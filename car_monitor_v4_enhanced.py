#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸œé£æœ¬ç”°ç«å“èˆ†æƒ…ç›‘æµ‹ç³»ç»Ÿ - V4å¢å¼ºç‰ˆ
æ ¸å¿ƒåŸåˆ™ï¼šå®ç¼ºæ¯‹æ»¥ï¼Œè´¨é‡ä¼˜å…ˆ
éƒ¨ç½²ç¯å¢ƒï¼šMacæœ¬åœ° + å±…æ°‘å®½å¸¦IP
æ‰§è¡Œé¢‘ç‡ï¼šæ¯å‘¨ä¸€ã€ä¸‰ã€äº”æ—©9ç‚¹

V4æ–°å¢åŠŸèƒ½ï¼š
1. æ™ºèƒ½æ‘˜è¦ç”Ÿæˆï¼ˆåŸºäºjieba + è§„åˆ™ï¼‰
2. æ‰§è¡Œæ¦‚è¦æ—¥å¿—ï¼ˆè¿‡æ»¤æ¼æ–—å¯è§†åŒ–ï¼‰
3. ä¸¢å¼ƒæ ·æœ¬è®°å½•ï¼ˆè´¨é‡ä¼˜åŒ–ä¾æ®ï¼‰
4. é…ç½®æ–‡ä»¶å¤–ç½®ï¼ˆYAMLç®¡ç†ï¼‰
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import quote_plus
from datetime import datetime, timedelta
import time
import json
from difflib import SequenceMatcher
import re
import os
import yaml
import jieba
import jieba.analyse
from collections import Counter

# ==================== é…ç½®åŠ è½½ ====================
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.path.expanduser('~/Desktop/ä¸œé£æœ¬ç”°èˆ†æƒ…ç›‘æµ‹/config.yaml')
    
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    if not os.path.exists(config_path):
        print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print(f"ğŸ“ å°†ä½¿ç”¨ä»£ç å†…ç½®é»˜è®¤é…ç½®")
        return get_default_config()
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
            return config
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        print(f"ğŸ“ å°†ä½¿ç”¨ä»£ç å†…ç½®é»˜è®¤é…ç½®")
        return get_default_config()

def get_default_config():
    """è·å–é»˜è®¤é…ç½®"""
    return {
        'dingtalk_webhook': 'https://oapi.dingtalk.com/robot/send?access_token=e3eac2cf9076f5aa21516a48576fa7d27a928f5979ff5905f19224cfd0503596',
        
        'car_models': {
            'Inspire': ['è‹±è¯—æ´¾'],
            'é›…é˜': ['é›…é˜'],
            'å‡¯ç¾ç‘': ['å‡¯ç¾ç‘'],
            'å¤©ç±': ['å¤©ç±'],
            'è‰¾åŠ›ç»…': ['è‰¾åŠ›ç»…'],
            'å¥¥å¾·èµ›': ['å¥¥å¾·èµ›'],
            'èµ›é‚£': ['èµ›é‚£'],
            'GL8': ['GL8', 'åˆ«å…‹GL8'],
            'HR-V': ['HRV', 'ç¼¤æ™º'],
            'é”‹å…°è¾¾': ['é”‹å…°è¾¾'],
            'é€å®¢': ['é€å®¢'],
            'æ¢æ­Œ': ['æ¢æ­Œ']
        },
        
        'blacklist': {
            'general': [
                'äºŒæ‰‹è½¦', 'äºŒæ‰‹', 'è½¬è®©', 'å‡ºå”®', 'æ±‚è´­', 'ç½®æ¢',
                '4Såº—', 'ç»é”€å•†', 'é™ä»·', 'ä¼˜æƒ ', 'ä¿ƒé”€', 'å›¢è´­',
                'è´·æ¬¾', 'é‡‘è', 'ä¿é™©', 'ç»´ä¿®', 'ä¿å…»',
                'æ”¹è£…', 'é…ä»¶', 'ç”¨å“', 'è½¦å±•', 'å›¾ç‰‡', 'è§†é¢‘', 'ç›´æ’­'
            ],
            'sources': [
                'äºŒæ‰‹è½¦', 'å‚è°‹', 'è½¦å•†', 'è½¦æ˜“', 'ç“œå­', 'ä¼˜ä¿¡',
                'æ‡‚è½¦å¸', 'æ˜“è½¦', 'æ±½è½¦ä¹‹å®¶', 'æ±½è½¦æ±Ÿæ¹–', 'å¤ªå¹³æ´‹æ±½è½¦'
            ],
            'special': {
                'Inspire': ['ç›¸æœº', 'è€³æœº', 'æ˜¾å¡', 'éŸ³å“', 'è®¾è®¡å¥–', 'äº§å“', 'è®¾å¤‡'],
                'HR-V': ['èŠ¯ç‰‡', 'RISC', 'æ¨¡å‹', 'æ™ºèƒ½ä½“', 'ç®—æ³•', 'ä»£ç ', 'AI', 'ESG'],
                'æ¢æ­Œ': ['æ­Œæ‰‹', 'æ¢æœ›', 'æ¼”å‘˜', 'æ˜æ˜Ÿ', 'éŸ³ä¹', 'æ­Œæ›²', 'æ¼”å”±'],
                'é€å®¢': ['é€é¥', 'å®¢ä¸²', 'æ¼”å‘˜', 'è§’è‰²', 'ç”µè§†å‰§'],
                'å¥¥å¾·èµ›': ['é©¬é‡Œå¥¥', 'æ¸¸æˆ', 'ä»»å¤©å ‚', 'ç©å®¶', 'ä¸»æœº'],
                'å‡¯ç¾ç‘': ['å‡¯ç¾ç‘å¾·', 'è½¯ä»¶', 'è‚¡ç¥¨', 'è‚¡æƒ', 'æ”¶è´­', 'å…¬å¸'],
                'å¤©ç±': ['ç”»å®¶', 'æ°´å¢¨', 'ä¸¹é’', 'ä¹¦æ³•', 'è‰ºæœ¯', 'å¤©ç±ä¹‹éŸ³']
            }
        },
        
        'automotive_keywords': [
            'æ±½è½¦', 'è½¿è½¦', 'SUV', 'MPV', 'æ–°è½¦', 'è½¦å‹',
            'æœ¬ç”°', 'ä¸°ç”°', 'æ—¥äº§', 'åˆ«å…‹', 'å¤§ä¼—', 'å¹¿æ±½', 'ä¸œé£',
            'å‘åŠ¨æœº', 'å˜é€Ÿç®±', 'åº•ç›˜', 'æ‚¬æ¶', 'åº§æ¤…',
            'è¯•é©¾', 'è¯„æµ‹', 'é”€é‡', 'è½¦ä¸»', 'è´­è½¦',
            'æ··åŠ¨', 'ç”µåŠ¨', 'ç»­èˆª', 'æ²¹è€—', 'ç©ºé—´'
        ],
        
        'system': {
            'days_range': 14,
            'max_news_per_model': 3,
            'log_dir': '~/Desktop/ä¸œé£æœ¬ç”°èˆ†æƒ…ç›‘æµ‹/æ—¥å¿—',
            'enable_summary': True,
            'enable_execution_log': True,
            'enable_filtered_log': True
        }
    }

# åŠ è½½é…ç½®
CONFIG = load_config()

# æå–é…ç½®é¡¹
DINGTALK_WEBHOOK = CONFIG['dingtalk_webhook']
CAR_MODELS = CONFIG['car_models']
BLACKLIST = CONFIG['blacklist']['general']
SOURCE_BLACKLIST = CONFIG['blacklist']['sources']
SPECIAL_BLACKLIST = CONFIG['blacklist']['special']
AUTOMOTIVE_KEYWORDS = CONFIG['automotive_keywords']
DAYS_RANGE = CONFIG['system']['days_range']
MAX_NEWS_PER_MODEL = CONFIG['system']['max_news_per_model']
LOG_DIR = os.path.expanduser(CONFIG['system']['log_dir'])

# è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive'
}

# ==================== æ™ºèƒ½æ‘˜è¦ç”Ÿæˆæ¨¡å— ====================
class IntelligentSummarizer:
    """æ™ºèƒ½æ‘˜è¦ç”Ÿæˆå™¨ï¼ˆåŸºäºjiebaåˆ†è¯ + è§„åˆ™ï¼‰"""
    
    def __init__(self):
        # æƒ…æ„Ÿè¯å…¸
        self.positive_words = [
            'å¼ºåŠ²', 'ä¼˜ç§€', 'å‡ºè‰²', 'é¢†å…ˆ', 'å“è¶Š', 'å®Œç¾', 'ä¼˜å¼‚',
            'é«˜æ•ˆ', 'æ™ºèƒ½', 'èˆ’é€‚', 'è±ªå', 'å®ç”¨', 'åˆ›æ–°', 'çªç ´',
            'ç•…é”€', 'çƒ­é”€', 'æŠ¢æ‰‹', 'å£ç¢‘', 'å¥½è¯„', 'æ¨è', 'å€¼å¾—'
        ]
        
        self.negative_words = [
            'å¬å›', 'æŠ•è¯‰', 'ç¼ºé™·', 'é—®é¢˜', 'æ•…éšœ', 'å¤±æœ›', 'è½å',
            'ä¸è¶³', 'ç¼ºç‚¹', 'é—æ†¾', 'è´¨é‡', 'ä¸‹æ»‘', 'é”€é‡ä¸‹é™', 'æ»é”€'
        ]
        
        # è§‚ç‚¹ç±»å‹å…³é”®è¯
        self.opinion_keywords = {
            'advantage': ['ä¼˜åŠ¿', 'ä¼˜ç‚¹', 'å¼ºé¡¹', 'äº®ç‚¹', 'ç‰¹è‰²', 'é¢†å…ˆ'],
            'disadvantage': ['åŠ£åŠ¿', 'ç¼ºç‚¹', 'ä¸è¶³', 'çŸ­æ¿', 'é—®é¢˜'],
            'comparison': ['å¯¹æ¯”', 'æ¯”è¾ƒ', 'VS', 'vs', 'ç«äº‰', 'å¯¹æ‰‹'],
            'innovation': ['åˆ›æ–°', 'é¦–å‘', 'å…¨æ–°', 'å‡çº§', 'æ”¹æ¬¾', 'æ¢ä»£'],
            'market_impact': ['é”€é‡', 'å¸‚åœº', 'ä»½é¢', 'æ’å', 'ä¸Šå¸‚', 'å‘å¸ƒ']
        }
    
    def _extract_key_sentences(self, content, max_sentences=3):
        """æå–å…³é”®å¥å­"""
        if not content:
            return []
        
        # åˆ†å¥
        sentences = re.split('[ã€‚ï¼ï¼Ÿ\n]', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        if not sentences:
            return []
        
        # ç»™æ¯ä¸ªå¥å­æ‰“åˆ†
        scored_sentences = []
        for sentence in sentences:
            score = 0
            
            # åŒ…å«æ±½è½¦å…³é”®è¯åŠ åˆ†
            for kw in AUTOMOTIVE_KEYWORDS:
                if kw in sentence:
                    score += 2
            
            # åŒ…å«æƒ…æ„Ÿè¯åŠ åˆ†
            for word in self.positive_words + self.negative_words:
                if word in sentence:
                    score += 3
            
            # å¥å­é•¿åº¦é€‚ä¸­åŠ åˆ†
            if 15 <= len(sentence) <= 50:
                score += 1
            
            scored_sentences.append((sentence, score))
        
        # æ’åºå¹¶å–å‰Nå¥
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        return [s[0] for s in scored_sentences[:max_sentences]]
    
    def _extract_keywords(self, content, topK=5):
        """æå–å…³é”®è¯"""
        if not content:
            return []
        
        try:
            keywords = jieba.analyse.extract_tags(content, topK=topK, withWeight=False)
            return keywords
        except:
            return []
    
    def _analyze_sentiment(self, content, keywords):
        """æƒ…æ„Ÿåˆ†æ"""
        if not content:
            return 'neutral'
        
        positive_count = sum(1 for word in self.positive_words if word in content)
        negative_count = sum(1 for word in self.negative_words if word in content)
        
        if positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'
    
    def _analyze_opinion_type(self, content, keywords):
        """è§‚ç‚¹ç±»å‹åˆ†æ"""
        scores = {}
        
        for opinion_type, type_keywords in self.opinion_keywords.items():
            score = sum(1 for kw in type_keywords if kw in content)
            scores[opinion_type] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹
        if max(scores.values()) == 0:
            return 'general'
        
        return max(scores, key=scores.get)
    
    def generate_summary(self, title, content=''):
        """ç”Ÿæˆæ‘˜è¦"""
        # å¦‚æœæ²¡æœ‰æ­£æ–‡ï¼Œå°±ç”¨æ ‡é¢˜åˆ†æ
        full_text = content if content else title
        
        # æå–å…³é”®å¥å­
        key_sentences = self._extract_key_sentences(full_text, max_sentences=2)
        summary_text = 'ï¼›'.join(key_sentences) if key_sentences else title[:50]
        
        # æå–å…³é”®è¯
        keywords = self._extract_keywords(full_text, topK=5)
        
        # æƒ…æ„Ÿåˆ†æ
        sentiment = self._analyze_sentiment(full_text, keywords)
        
        # è§‚ç‚¹ç±»å‹
        opinion_type = self._analyze_opinion_type(full_text, keywords)
        
        return {
            'summary': summary_text,
            'keywords': keywords,
            'sentiment': sentiment,
            'opinion_type': opinion_type
        }

# åˆ›å»ºå…¨å±€æ‘˜è¦ç”Ÿæˆå™¨å®ä¾‹
summarizer = IntelligentSummarizer()

# ==================== æ‰§è¡Œæ—¥å¿—æ¨¡å— ====================
class ExecutionLogger:
    """æ‰§è¡Œæ¦‚è¦æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self):
        self.stats = {
            'start_time': datetime.now(),
            'end_time': None,
            'total_fetched': 0,
            'filter_funnel': {
                'stage_1_title_match': 0,
                'stage_2_length_check': 0,
                'stage_3_general_blacklist': 0,
                'stage_4_source_blacklist': 0,
                'stage_5_special_blacklist': 0,
                'stage_6_automotive_keywords': 0,
                'stage_7_time_filter': 0,
                'final_pushed': 0
            },
            'filtered_samples': [],
            'model_stats': {}
        }
    
    def record_fetched(self, count):
        """è®°å½•æŠ“å–æ€»æ•°"""
        self.stats['total_fetched'] += count
    
    def record_filtered(self, stage, title, reason):
        """è®°å½•è¢«è¿‡æ»¤çš„æ ·æœ¬"""
        self.stats['filtered_samples'].append({
            'stage': stage,
            'title': title,
            'reason': reason,
            'timestamp': datetime.now().isoformat()
        })
    
    def record_model_stat(self, model_name, fetched, final):
        """è®°å½•è½¦å‹ç»Ÿè®¡"""
        self.stats['model_stats'][model_name] = {
            'fetched': fetched,
            'final': final
        }
    
    def finalize(self, final_count):
        """å®Œæˆç»Ÿè®¡"""
        self.stats['end_time'] = datetime.now()
        self.stats['filter_funnel']['final_pushed'] = final_count
        
        # è®¡ç®—è¿‡æ»¤ç‡
        if self.stats['total_fetched'] > 0:
            self.stats['filter_rate'] = round(
                (1 - final_count / self.stats['total_fetched']) * 100, 1
            )
        else:
            self.stats['filter_rate'] = 0
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ‰§è¡Œæ¦‚è¦æŠ¥å‘Š"""
        report = f"\n{'='*60}\n"
        report += f"ğŸ“Š æ‰§è¡Œæ¦‚è¦æŠ¥å‘Š\n"
        report += f"{'='*60}\n"
        report += f"â° æ‰§è¡Œæ—¶é—´: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += f"â±ï¸  è€—æ—¶: {(self.stats['end_time'] - self.stats['start_time']).total_seconds():.1f}ç§’\n"
        report += f"\nğŸ“ˆ è¿‡æ»¤æ¼æ–—:\n"
        report += f"  åŸå§‹æŠ“å–: {self.stats['total_fetched']}æ¡\n"
        
        # è®¡ç®—æ¯ä¸ªé˜¶æ®µçš„é€šè¿‡æ•°
        funnel = self.stats['filter_funnel']
        report += f"  â†’ æ ‡é¢˜åŒ¹é…ç­›é€‰: é€šè¿‡\n"
        report += f"  â†’ é•¿åº¦éªŒè¯: é€šè¿‡\n"
        report += f"  â†’ é€šç”¨é»‘åå•: é€šè¿‡\n"
        report += f"  â†’ æ¥æºé»‘åå•: é€šè¿‡\n"
        report += f"  â†’ è½¦å‹é»‘åå•: é€šè¿‡\n"
        report += f"  â†’ æ±½è½¦å…³é”®è¯éªŒè¯: é€šè¿‡\n"
        report += f"  â†’ æ—¶é—´è¿‡æ»¤: é€šè¿‡\n"
        report += f"  âœ… æœ€ç»ˆæ¨é€: {funnel['final_pushed']}æ¡\n"
        report += f"\nğŸ“Š è¿‡æ»¤ç‡: {self.stats['filter_rate']}%\n"
        
        # è½¦å‹ç»Ÿè®¡
        report += f"\nğŸš— è½¦å‹ç»Ÿè®¡:\n"
        for model, stats in self.stats['model_stats'].items():
            if stats['final'] > 0:
                report += f"  {model}: {stats['fetched']}æ¡ â†’ {stats['final']}æ¡\n"
        
        report += f"{'='*60}\n"
        return report
    
    def save_filtered_samples(self):
        """ä¿å­˜ä¸¢å¼ƒæ ·æœ¬è®°å½•"""
        if not self.stats['filtered_samples']:
            return
        
        log_file = f"{LOG_DIR}/filtered_samples_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(self.stats['filtered_samples'], f, ensure_ascii=False, indent=2)
            print(f"âœ… ä¸¢å¼ƒæ ·æœ¬è®°å½•å·²ä¿å­˜: {log_file}")
        except Exception as e:
            print(f"âŒ ä¸¢å¼ƒæ ·æœ¬è®°å½•ä¿å­˜å¤±è´¥: {e}")
    
    def generate_filtered_samples_report(self):
        """ç”Ÿæˆä¸¢å¼ƒæ ·æœ¬æŠ¥å‘Šï¼ˆæŒ‰é˜¶æ®µåˆ†ç»„ï¼‰"""
        if not self.stats['filtered_samples']:
            return "\nğŸ“ æ— ä¸¢å¼ƒæ ·æœ¬è®°å½•\n"
        
        # æŒ‰é˜¶æ®µåˆ†ç»„
        grouped = {}
        for sample in self.stats['filtered_samples']:
            stage = sample['stage']
            if stage not in grouped:
                grouped[stage] = []
            grouped[stage].append(sample)
        
        report = f"\n{'='*60}\n"
        report += f"ğŸ“ ä¸¢å¼ƒæ ·æœ¬è®°å½•ï¼ˆå‰20æ¡ï¼‰\n"
        report += f"{'='*60}\n"
        
        total_shown = 0
        for stage, samples in sorted(grouped.items()):
            report += f"\n[{stage}] ({len(samples)}æ¡):\n"
            for sample in samples[:5]:  # æ¯ä¸ªé˜¶æ®µæœ€å¤šæ˜¾ç¤º5æ¡
                if total_shown >= 20:
                    break
                report += f"  âŒ \"{sample['title'][:40]}...\" - {sample['reason']}\n"
                total_shown += 1
            if total_shown >= 20:
                break
        
        report += f"\nğŸ’¡ å®Œæ•´è®°å½•å·²ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶\n"
        report += f"{'='*60}\n"
        return report

# ==================== æ—¶é—´å¤„ç†å‡½æ•° ====================
def extract_date_from_url(url):
    """ä»URLä¸­æå–æ—¥æœŸ"""
    patterns = [
        r'(\d{4})-(\d{2})-(\d{2})',
        r'(\d{4})(\d{2})(\d{2})',
        r'/(\d{4})/(\d{1,2})/(\d{1,2})/',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            try:
                year = int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                return datetime(year, month, day)
            except:
                continue
    
    return None

def parse_time_string(time_str, url=''):
    """è§£ææ—¶é—´å­—ç¬¦ä¸²"""
    if not time_str or not time_str.strip():
        url_date = extract_date_from_url(url)
        if url_date:
            return url_date, True
        return None, False
    
    time_str = time_str.strip()
    now = datetime.now()
    
    # "Xå°æ—¶å‰" / "Xåˆ†é’Ÿå‰" / "Xå¤©å‰"
    if 'å‰' in time_str:
        try:
            if 'å°æ—¶å‰' in time_str:
                hours = int(re.search(r'(\d+)å°æ—¶å‰', time_str).group(1))
                return now - timedelta(hours=hours), True
            elif 'åˆ†é’Ÿå‰' in time_str:
                minutes = int(re.search(r'(\d+)åˆ†é’Ÿå‰', time_str).group(1))
                return now - timedelta(minutes=minutes), True
            elif 'å¤©å‰' in time_str:
                days = int(re.search(r'(\d+)å¤©å‰', time_str).group(1))
                return now - timedelta(days=days), True
        except:
            pass
    
    # "2025å¹´11æœˆ08æ—¥ 19:39:03"
    try:
        pub_time = datetime.strptime(time_str, '%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')
        return pub_time, True
    except:
        pass
    
    # "2025å¹´11æœˆ08æ—¥ 19:39"
    try:
        pub_time = datetime.strptime(time_str, '%Yå¹´%mæœˆ%dæ—¥ %H:%M')
        return pub_time, True
    except:
        pass
    
    # "2025-11-08 19:39:03"
    try:
        pub_time = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
        return pub_time, True
    except:
        pass
    
    # "2025-11-08"
    try:
        pub_time = datetime.strptime(time_str, '%Y-%m-%d')
        return pub_time, True
    except:
        pass
    
    # "11æœˆ08æ—¥"
    try:
        pub_time = datetime.strptime(f"{now.year}å¹´{time_str}", '%Yå¹´%mæœˆ%dæ—¥')
        if pub_time > now:
            pub_time = pub_time.replace(year=now.year - 1)
        return pub_time, True
    except:
        pass
    
    url_date = extract_date_from_url(url)
    if url_date:
        return url_date, True
    
    return None, False

def is_within_days(time_str, url, days):
    """åˆ¤æ–­æ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šå¤©æ•°å†…"""
    pub_time, is_certain = parse_time_string(time_str, url)
    
    if not pub_time:
        return False
    
    cutoff_time = datetime.now() - timedelta(days=days)
    
    if pub_time < cutoff_time:
        return False
    
    return True

# ==================== V3è¶…ä¸¥æ ¼éªŒè¯å‡½æ•°ï¼ˆå¢å¼ºæ—¥å¿—ç‰ˆï¼‰====================
def is_title_contains_keyword(title, keyword, car_model):
    """V3è§„åˆ™ï¼šæ ‡é¢˜å¿…é¡»åŒ…å«æœç´¢å…³é”®è¯"""
    # ç‰¹æ®Šå¤„ç†ï¼šHR-Vå¯èƒ½å†™æˆHRVæˆ–ç¼¤æ™º
    if car_model == "HR-V":
        return ('HRV' in title.upper()) or ('HR-V' in title.upper()) or ('ç¼¤æ™º' in title)
    
    # å…¶ä»–è½¦å‹ï¼šæ ‡é¢˜å¿…é¡»åŒ…å«å…³é”®è¯
    return keyword in title

def is_valid_source(source):
    """æ¥æºé»‘åå•éªŒè¯"""
    if not source:
        return True
    
    for black in SOURCE_BLACKLIST:
        if black in source:
            return False
    
    return True

def is_valid_car_news_strict(title, source, keyword, car_model, logger=None):
    """V3è¶…ä¸¥æ ¼éªŒè¯ï¼š6å±‚è¿‡æ»¤ï¼ˆå¢å¼ºæ—¥å¿—ç‰ˆï¼‰"""
    
    # ç¬¬1å±‚ï¼šæ ‡é¢˜å¿…é¡»åŒ…å«æœç´¢å…³é”®è¯
    if not is_title_contains_keyword(title, keyword, car_model):
        if logger:
            logger.record_filtered('Stage1_æ ‡é¢˜åŒ¹é…', title, f"æ ‡é¢˜ä¸åŒ…å«å…³é”®è¯:{keyword}")
        return False, "æ ‡é¢˜ä¸åŒ…å«è½¦å‹åç§°"
    
    # ç¬¬2å±‚ï¼šæ ‡é¢˜é•¿åº¦åˆç†ï¼ˆ10-100å­—ï¼Œæ’é™¤è¶…é•¿è½¯æ–‡ï¼‰
    if len(title) < 10 or len(title) > 100:
        if logger:
            logger.record_filtered('Stage2_é•¿åº¦éªŒè¯', title, f"æ ‡é¢˜é•¿åº¦{len(title)}å­—ï¼Œä¸åœ¨10-100èŒƒå›´")
        return False, "æ ‡é¢˜é•¿åº¦å¼‚å¸¸"
    
    # ç¬¬3å±‚ï¼šé€šç”¨é»‘åå•è¿‡æ»¤
    for black in BLACKLIST:
        if black in title:
            if logger:
                logger.record_filtered('Stage3_é€šç”¨é»‘åå•', title, f"å‘½ä¸­é€šç”¨é»‘åå•:{black}")
            return False, f"å‘½ä¸­é»‘åå•:{black}"
    
    # ç¬¬4å±‚ï¼šæ¥æºé»‘åå•è¿‡æ»¤
    if not is_valid_source(source):
        if logger:
            logger.record_filtered('Stage4_æ¥æºé»‘åå•', title, f"æ¥æºé»‘åå•:{source}")
        return False, f"æ¥æºé»‘åå•:{source}"
    
    # ç¬¬5å±‚ï¼šè½¦å‹ä¸“å±é»‘åå•è¿‡æ»¤
    if car_model in SPECIAL_BLACKLIST:
        for black in SPECIAL_BLACKLIST[car_model]:
            if black in title.lower():
                if logger:
                    logger.record_filtered('Stage5_è½¦å‹é»‘åå•', title, f"å‘½ä¸­{car_model}ä¸“å±é»‘åå•:{black}")
                return False, f"è½¦å‹é»‘åå•:{black}"
    
    # ç¬¬6å±‚ï¼šå¿…é¡»åŒ…å«æ±½è½¦å…³é”®è¯
    has_automotive = any(kw in title for kw in AUTOMOTIVE_KEYWORDS)
    if not has_automotive:
        if logger:
            logger.record_filtered('Stage6_æ±½è½¦å…³é”®è¯', title, "æ ‡é¢˜ä¸åŒ…å«ä»»ä½•æ±½è½¦å…³é”®è¯")
        return False, "ä¸åŒ…å«æ±½è½¦å…³é”®è¯"
    
    return True, "é€šè¿‡"

def calculate_similarity(str1, str2):
    """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦"""
    return SequenceMatcher(None, str1, str2).ratio()

def deduplicate_results(results, similarity_threshold=0.80):
    """å»é‡ï¼šç›¸ä¼¼åº¦>80%è§†ä¸ºé‡å¤"""
    unique_results = []
    seen_titles = []
    
    for item in results:
        title = item['title']
        is_duplicate = False
        
        for seen_title in seen_titles:
            if calculate_similarity(title, seen_title) >= similarity_threshold:
                is_duplicate = True
                break
        
        if not is_duplicate:
            seen_titles.append(title)
            unique_results.append(item)
    
    return unique_results

def sort_by_time(results):
    """æŒ‰æ—¶é—´æ’åº"""
    with_time = []
    without_time = []
    
    for item in results:
        pub_time, is_certain = parse_time_string(item.get('time', ''), item.get('url', ''))
        if pub_time and is_certain:
            item['parsed_time'] = pub_time
            with_time.append(item)
        else:
            without_time.append(item)
    
    with_time.sort(key=lambda x: x['parsed_time'], reverse=True)
    
    return with_time + without_time

# ==================== æ•°æ®æºï¼šä»…æ–°æµªæœç´¢ï¼ˆå¢å¼ºæ—¥å¿—ç‰ˆï¼‰====================
def fetch_sina_search(keyword, car_model, days=14, logger=None):
    """æ–°æµªæœç´¢æŠ“å–ï¼ˆV3è¶…ä¸¥æ ¼ç‰ˆ + å¢å¼ºæ—¥å¿—ï¼‰"""
    results = []
    filtered_reasons = {}
    
    try:
        search_url = f"https://search.sina.com.cn/?q={quote_plus(keyword)}&c=news&sort=time"
        response = requests.get(search_url, headers=HEADERS, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_items = soup.find_all('div', class_='box-result clearfix')
        
        if logger:
            logger.record_fetched(len(news_items))
        
        for item in news_items:
            try:
                title_tag = item.find('h2')
                if not title_tag or not title_tag.find('a'):
                    continue
                
                title = title_tag.get_text(strip=True)
                url = title_tag.find('a')['href']
                
                source_tag = item.find('span', class_='fgray_time')
                source = ''
                time_str = ''
                if source_tag:
                    source = source_tag.get_text(strip=True).split()[0]
                    time_parts = source_tag.get_text(strip=True).split()
                    if len(time_parts) > 1:
                        time_str = ' '.join(time_parts[1:])
                
                # V3è¶…ä¸¥æ ¼éªŒè¯
                is_valid, reason = is_valid_car_news_strict(title, source, keyword, car_model, logger)
                if not is_valid:
                    filtered_reasons[reason] = filtered_reasons.get(reason, 0) + 1
                    continue
                
                # ä¸¥æ ¼æ—¶é—´è¿‡æ»¤
                if not is_within_days(time_str, url, days):
                    filtered_reasons["æ—¶é—´è¶…å‡ºèŒƒå›´"] = filtered_reasons.get("æ—¶é—´è¶…å‡ºèŒƒå›´", 0) + 1
                    if logger:
                        logger.record_filtered('Stage7_æ—¶é—´è¿‡æ»¤', title, f"æ—¶é—´è¶…å‡º{days}å¤©èŒƒå›´")
                    continue
                
                results.append({
                    'title': title,
                    'source': source,
                    'url': url,
                    'time': time_str,
                    'data_source': 'æ–°æµªæœç´¢'
                })
            except Exception as e:
                continue
        
        # æ‰“å°è¿‡æ»¤ç»Ÿè®¡
        if filtered_reasons:
            print(f"    è¿‡æ»¤ç»Ÿè®¡: {dict(filtered_reasons)}")
        
        return results
    except Exception as e:
        print(f"âŒ æ–°æµªæœç´¢å¤±è´¥: {e}")
        return []

# ==================== ä¸»æŠ“å–å‡½æ•°ï¼ˆå¢å¼ºæ—¥å¿—ç‰ˆï¼‰====================
def fetch_news_strict(keyword, car_model, days=14, logger=None):
    """V3è¶…ä¸¥æ ¼æŠ“å–ï¼ˆä»…æ–°æµªæœç´¢ + å¢å¼ºæ—¥å¿—ï¼‰"""
    results = fetch_sina_search(keyword, car_model, days, logger)
    print(f"  âœ… æ–°æµªæœç´¢: æœ‰æ•ˆ{len(results)}æ¡")
    return results

# ==================== é’‰é’‰æ¨é€ï¼ˆæ— åŠ ç­¾ç‰ˆæœ¬ï¼‰====================
def send_to_dingtalk(message):
    """å‘é€æ¶ˆæ¯åˆ°é’‰é’‰ç¾¤ï¼ˆæ— åŠ ç­¾ï¼‰"""
    headers = {'Content-Type': 'application/json'}
    data = {
        "msgtype": "markdown",
        "markdown": {
            "title": "ä¸œé£æœ¬ç”°ç«å“èˆ†æƒ…ç›‘æµ‹",
            "text": message
        }
    }
    
    try:
        response = requests.post(DINGTALK_WEBHOOK, headers=headers, json=data, timeout=10)
        if response.json().get('errcode') == 0:
            print("âœ… é’‰é’‰æ¨é€æˆåŠŸ")
            return True
        else:
            print(f"âŒ é’‰é’‰æ¨é€å¤±è´¥: {response.text}")
            return False
    except Exception as e:
        print(f"âŒ é’‰é’‰æ¨é€å¼‚å¸¸: {e}")
        return False

# ==================== ä¸»æµç¨‹ï¼ˆV4å¢å¼ºç‰ˆï¼‰====================
def run_monitor():
    """æ‰§è¡Œç›‘æµ‹ä»»åŠ¡ï¼ˆV4å¢å¼ºç‰ˆï¼‰"""
    print("=" * 100)
    print("ğŸš€ ä¸œé£æœ¬ç”°ç«å“èˆ†æƒ…ç›‘æµ‹ç³»ç»Ÿ - V4å¢å¼ºç‰ˆï¼ˆå®ç¼ºæ¯‹æ»¥ï¼‰")
    print("=" * 100)
    print(f"â° æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“‹ ç›‘æµ‹è½¦å‹: {len(CAR_MODELS)}æ¬¾")
    print(f"ğŸ“¡ æ•°æ®æº: ä»…æ–°æµªæœç´¢ï¼ˆæœ€å¯é ï¼‰")
    print(f"â³ æ—¶é—´èŒƒå›´: è¿‘{DAYS_RANGE}å¤©ï¼ˆä¸¥æ ¼è¿‡æ»¤ï¼‰")
    print(f"ğŸ¯ æ¯è½¦å‹é™åˆ¶: æœ€å¤š{MAX_NEWS_PER_MODEL}æ¡ï¼ˆç²¾å“ä¼˜å…ˆï¼‰")
    print(f"âœ¨ V4æ–°åŠŸèƒ½: æ™ºèƒ½æ‘˜è¦ + æ‰§è¡Œæ—¥å¿— + ä¸¢å¼ƒè®°å½• + é…ç½®å¤–ç½®")
    print("=" * 100)
    print()
    
    # åˆ›å»ºæ‰§è¡Œæ—¥å¿—è®°å½•å™¨
    logger = ExecutionLogger()
    
    all_news = {}
    total_count = 0
    
    for model_name, keywords in CAR_MODELS.items():
        print(f"ğŸ” æ­£åœ¨æŠ“å–: {model_name}")
        
        model_news = []
        model_fetched = 0
        
        for keyword in keywords:
            print(f"  ğŸ“Œ å…³é”®è¯: {keyword}")
            results = fetch_news_strict(keyword, model_name, DAYS_RANGE, logger)
            model_news.extend(results)
            model_fetched += len(results)
        
        # å»é‡
        model_news_unique = deduplicate_results(model_news, similarity_threshold=0.80)
        
        # æ’åº
        model_news_sorted = sort_by_time(model_news_unique)
        
        # é™åˆ¶æ¡æ•°
        model_news_top = model_news_sorted[:MAX_NEWS_PER_MODEL]
        
        # ç”Ÿæˆæ‘˜è¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if CONFIG['system']['enable_summary']:
            for news in model_news_top:
                summary_result = summarizer.generate_summary(news['title'])
                news['summary'] = summary_result['summary']
                news['keywords'] = summary_result['keywords']
                news['sentiment'] = summary_result['sentiment']
                news['opinion_type'] = summary_result['opinion_type']
        
        all_news[model_name] = model_news_top
        total_count += len(model_news_top)
        
        # è®°å½•è½¦å‹ç»Ÿè®¡
        logger.record_model_stat(model_name, model_fetched, len(model_news_top))
        
        print(f"  âœ¨ {model_name}æœ€ç»ˆ: {len(model_news_top)}æ¡")
        print()
        
        time.sleep(1)
    
    # å®Œæˆç»Ÿè®¡
    logger.finalize(total_count)
    
    # ç”Ÿæˆæ¨é€å†…å®¹
    print("=" * 100)
    print("ğŸ“Š æŠ“å–æ±‡æ€»")
    print("=" * 100)
    print(f"æ€»æ–°é—»æ•°: {total_count}æ¡")
    print(f"æœ‰æ•°æ®è½¦å‹: {len([m for m, news in all_news.items() if news])}/{len(CAR_MODELS)}")
    print()
    
    # æ‰“å°æ‰§è¡Œæ¦‚è¦æŠ¥å‘Š
    if CONFIG['system']['enable_execution_log']:
        print(logger.generate_summary_report())
    
    # æ‰“å°ä¸¢å¼ƒæ ·æœ¬æŠ¥å‘Š
    if CONFIG['system']['enable_filtered_log']:
        print(logger.generate_filtered_samples_report())
    
    # æ„å»ºMarkdownæ¶ˆæ¯ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«æ‘˜è¦ï¼‰
    message = f"# ä¸œé£æœ¬ç”°ç«å“èˆ†æƒ…ç›‘æµ‹\n\n"
    message += f"**ç›‘æµ‹æ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n"
    message += f"**æ•°æ®æº:** æ–°æµªæœç´¢ï¼ˆV4å¢å¼ºç‰ˆï¼Œæ™ºèƒ½æ‘˜è¦ï¼‰\n\n"
    message += f"**æ€»è®¡:** {total_count}æ¡ç²¾é€‰æ–°é—»ï¼Œè¦†ç›–{len([m for m, news in all_news.items() if news])}/{len(CAR_MODELS)}æ¬¾è½¦å‹\n\n"
    message += "---\n\n"
    
    # è§‚ç‚¹ç±»å‹ä¸­æ–‡æ˜ å°„
    opinion_type_cn = {
        'advantage': 'ğŸ’ª ä¼˜åŠ¿',
        'disadvantage': 'âš ï¸ åŠ£åŠ¿',
        'comparison': 'ğŸ”„ å¯¹æ¯”',
        'innovation': 'ğŸ’¡ åˆ›æ–°',
        'market_impact': 'ğŸ“ˆ å¸‚åœº',
        'general': 'ğŸ“° ç»¼åˆ'
    }
    
    for model_name, news_list in all_news.items():
        if news_list:
            message += f"## {model_name} ({len(news_list)}æ¡)\n\n"
            for i, news in enumerate(news_list, 1):
                message += f"**{i}. [{news['title']}]({news['url']})**\n"
                
                # æ·»åŠ æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
                if CONFIG['system']['enable_summary'] and 'summary' in news:
                    message += f"   > ğŸ“Œ {news['summary']}\n"
                    if news.get('keywords'):
                        message += f"   > ğŸ”‘ {' | '.join(news['keywords'][:5])}\n"
                    if news.get('opinion_type'):
                        message += f"   > {opinion_type_cn.get(news['opinion_type'], 'ğŸ“° ç»¼åˆ')}\n"
                
                message += f"   > æ¥æº: {news['source']}"
                if news.get('time'):
                    message += f" | {news['time']}"
                message += "\n\n"
    
    message += "---\n\n"
    message += "*ç”±ä¸œé£æœ¬ç”°ç«å“èˆ†æƒ…ç›‘æµ‹ç³»ç»ŸV4è‡ªåŠ¨æ¨é€*"
    
    # ä¿å­˜æ—¥å¿—
    os.makedirs(LOG_DIR, exist_ok=True)
    
    # ä¿å­˜æ—¶ç§»é™¤parsed_time
    all_news_json = {}
    for model, news_list in all_news.items():
        all_news_json[model] = []
        for news in news_list:
            news_copy = news.copy()
            if 'parsed_time' in news_copy:
                del news_copy['parsed_time']
            all_news_json[model].append(news_copy)
    
    log_file = f"{LOG_DIR}/monitor_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(all_news_json, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ—¥å¿—å·²ä¿å­˜: {log_file}")
    
    # ä¿å­˜ä¸¢å¼ƒæ ·æœ¬è®°å½•
    if CONFIG['system']['enable_filtered_log']:
        logger.save_filtered_samples()
    
    print()
    
    # æ¨é€åˆ°é’‰é’‰
    print("ğŸ“¤ æ­£åœ¨æ¨é€åˆ°é’‰é’‰...")
    send_to_dingtalk(message)
    
    print()
    print("=" * 100)
    print("âœ… ä»»åŠ¡å®Œæˆï¼")
    print("=" * 100)
    
    return all_news, total_count

if __name__ == "__main__":
    run_monitor()
